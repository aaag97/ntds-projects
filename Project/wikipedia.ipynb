{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "wikipedia.set_lang('en') # setting wikipedia language\n",
    "import wikipediaapi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from time import sleep\n",
    "import sys\n",
    "import pageviewapi.period\n",
    "from random import sample \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import nltk # for nlp on articles\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAquisitionUtils():\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_category_members(category_members, level=0, max_level=1):\n",
    "        \"\"\"\n",
    "        Function to take all article in category (max_level control the depth of articles taken from the subcategories)\n",
    "        Arguments:\n",
    "        category_members - a list of category members\n",
    "        level - the level at which to start getting articles\n",
    "        max_level - the maximal level for the fetched articles\n",
    "        Returns:\n",
    "        list_articles - a list of the desired articles\n",
    "        \"\"\"\n",
    "        list_articles = []\n",
    "        for c in category_members.values():\n",
    "            if c.ns == 0:\n",
    "                list_articles.append(c) \n",
    "                #print(\"%s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n",
    "            elif level < max_level and c.ns == 14:\n",
    "                sub_list = []\n",
    "                sub_list = DataAquisitionUtils.fetch_category_members(c.categorymembers, level=level + 1, max_level=max_level)\n",
    "                list_articles = list_articles + sub_list\n",
    "        return list_articles\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperlinks_matrix(list_articles):\n",
    "        \"\"\"\n",
    "        Computes an adjacency matrix with the hyperlinks between the different articles in the argument given\n",
    "        Arguments:\n",
    "        list_articles - a list of articles for which to compute the hyperlink matrix\n",
    "        Returns:\n",
    "        matrix - a binary matrix A where A[i,j] = 1 if article i has a hyperlinkt to article j, and A[i,j] = 0 otherwise\n",
    "        \"\"\"\n",
    "        len_time = len(list_articles)*0.05\n",
    "        matrix = np.zeros((len(list_articles), len(list_articles)))\n",
    "        compt = 0\n",
    "        for article in list_articles:\n",
    "            for link in article.links:\n",
    "                for i in range(len(list_articles)):\n",
    "                    if (link == list_articles[i].title):\n",
    "                        matrix[compt,i] = 1\n",
    "            compt = compt + 1\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(compt/len_time), int(5*compt/len_time)))\n",
    "            sys.stdout.flush()\n",
    "            sleep(0.25)\n",
    "        return matrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def take_words(list_articles, stop_words, is_title = False):\n",
    "        \"\"\"\n",
    "        Function that tokenizes and returns all words in the list of articles given\n",
    "        Arguments:\n",
    "        list_articles - list of articles\n",
    "        is_title - whether the list contains pages or just strings of titles\n",
    "        Returns:\n",
    "        words_df - the words in the articles in a dataframe\n",
    "        \"\"\"\n",
    "        len_time = len(list_articles)*0.05\n",
    "        words_df = pd.DataFrame(columns=['article', 'words'])\n",
    "        for i in range(len(list_articles)):\n",
    "            try:\n",
    "                if (is_title == False):\n",
    "                    page = wikipedia.page(list_articles[i].title)\n",
    "                else:\n",
    "                    page = wikipedia.page(list_articles[i])\n",
    "            except wikipedia.DisambiguationError as e:\n",
    "                s = e.options\n",
    "                s = list(filter(lambda x : x != \"\", s))\n",
    "                try :\n",
    "                    page = wikipedia.page(s)\n",
    "                except wikipedia.DisambiguationError as e:\n",
    "                    pass\n",
    "            except wikipedia.PageError:\n",
    "                pass\n",
    "            words = word_tokenize(page.content)\n",
    "            words = [elem.lower() for elem in words]\n",
    "            words = [elem for elem in words if len(elem) > 1 and elem.isdigit() == False]\n",
    "            words_wostop = [x for x in words if x not in stop_words]\n",
    "            words_wostop = [elem.lower() for elem in words_wostop]\n",
    "            if (is_title == False):\n",
    "                words_df.loc[i] = [list_articles[i].title] + [words_wostop]\n",
    "            else:\n",
    "                words_df.loc[i] = [list_articles[i]] + [words_wostop]\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"[%-20s] %d%%\" % ('='*int((i+1)/len_time), int(5*(i+1)/len_time)))\n",
    "            sys.stdout.flush()\n",
    "            sleep(0.25)\n",
    "        return words_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def page_view(list_articles): \n",
    "        len_time = len(list_articles)*0.05\n",
    "        view_df = pd.DataFrame(columns=['article', 'views'])\n",
    "        for i in range(len(list_articles)):\n",
    "            try: \n",
    "                view_df.loc[i] = [list_articles[i].title] + [pageviewapi.period.sum_last('en.wikipedia', list_articles[i].title, last=30,\n",
    "                        access='all-access', agent='all-agents')]\n",
    "            except pageviewapi.client.ZeroOrDataNotLoadedException as e:\n",
    "                view_df.loc[i] = 0\n",
    "            except wikipedia.PageError:\n",
    "                pass\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"[%-20s] %d%%\" % ('='*int((i+1)/len_time), int(5*(i+1)/len_time)))\n",
    "            sys.stdout.flush()\n",
    "            sleep(0.25)\n",
    "        return view_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPUtilities():\n",
    "    \n",
    "    @staticmethod\n",
    "    def TF_IDF(words_df, nb_words, has_mywords = False, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Function to get the words with the top TF-IDF scores\n",
    "        Arguments:\n",
    "        words_df - the dataframe consisting of the words to be considered\n",
    "        nb_words - the number of words we would like the function to return\n",
    "        Returns:\n",
    "        df - a dataframe containing the words with the best TF-IDF scores\n",
    "        \"\"\"\n",
    "        #remove strings like \"10,000\"\n",
    "        #religion_df['words'] = religion_df['words'].apply(lambda word_list: list(filter(lambda word: (',' not in word), word_list)))\n",
    "        #only keep strings that contain alphabet chars\n",
    "        words_df['words'] = words_df['words'].apply(lambda word_list: list(filter(lambda word: word.isalpha(), word_list)))\n",
    "        # create a column with all words concatenated\n",
    "        words_df['words_string'] = words_df['words'].apply(lambda words_list: \" \".join(words_list))\n",
    "\n",
    "        cv=CountVectorizer()\n",
    "        word_count_vector=cv.fit_transform(words_df['words_string'])\n",
    "\n",
    "        tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "        # count matrix\n",
    "        count_vector=cv.transform(words_df['words_string'])\n",
    "\n",
    "        # tf-idf scores\n",
    "        tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "\n",
    "        feature_names = cv.get_feature_names()\n",
    "        \n",
    "        dense = tf_idf_vector.todense()\n",
    "        denselist = dense.tolist()\n",
    "                \n",
    "        #tf_idf = matrix where rows are articles and columns are words and values are tfidf score for word in article\n",
    "        tf_idf = pd.DataFrame(data=denselist, columns=feature_names, index=words_df['article'])\n",
    "        \n",
    "        if (has_mywords == False):\n",
    "            #these are the words that have the biggest tfidf score\n",
    "            important_words = pd.DataFrame(tf_idf.sum(axis=0, numeric_only=True).sort_values(ascending = False),\\\n",
    "                                      columns=['tfidf']).head(nb_words).index.values\n",
    "        else:\n",
    "            important_words = args[0]\n",
    "        \n",
    "        #only keep the most important words in matrix\n",
    "        tf_idf = tf_idf[important_words]\n",
    "\n",
    "        return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the articles of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquire = DataAquisitionUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('en') # getting articles in english\n",
    "# fetching the articles for categories of interest\n",
    "religion_page = wiki_wiki.page(\"Category:Religion\")\n",
    "science_page = wiki_wiki.page(\"Category:Science\")\n",
    "religion_articles = aquire.fetch_category_members(religion_page.categorymembers)\n",
    "science_articles = aquire.fetch_category_members(science_page.categorymembers)\n",
    "list_articles = religion_articles + science_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will balance the number of articles in each section "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperlink matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the adjacency matrix of the hyperlinks between articles. That is if one article Ai links to another article Aj there will be an 1 in the ith row and jth column of the matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=                   ] 5%"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='en.wikipedia.org', port=443): Read timed out. (read timeout=10.0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    420\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1052\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mtimeout\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    719\u001b[0m             retries = retries.increment(\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             )\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    330\u001b[0m             raise ReadTimeoutError(\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read timed out. (read timeout=%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             )\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Read timed out. (read timeout=10.0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7e57d61f3340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# EXECUTE IF YOU DO NOT HAVE CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlist_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreligion_articles\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscience_articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0madjacency_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maquire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperlinks_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnum_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacency_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of edges in the feature graph: {num_edges}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6d21b1a48c20>\u001b[0m in \u001b[0;36mhyperlinks_matrix\u001b[0;34m(list_articles)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mcompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_articles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/wikipediaapi/__init__.py\u001b[0m in \u001b[0;36mlinks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \"\"\"\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_called\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'links'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'links'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/wikipediaapi/__init__.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self, call)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'WikipediaPage'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_called\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/wikipediaapi/__init__.py\u001b[0m in \u001b[0;36mlinks\u001b[0;34m(self, page, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         raw = self._query(\n\u001b[1;32m    402\u001b[0m             \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mused_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_common_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/wikipediaapi/__init__.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, page, params)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         )\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Read timed out. (read timeout=10.0)"
     ]
    }
   ],
   "source": [
    "# EXECUTE IF YOU DO NOT HAVE CSV\n",
    "list_articles = religion_articles + science_articles\n",
    "adjacency_matrix = aquire.hyperlinks_matrix(list_articles)\n",
    "num_edges = np.count_nonzero(adjacency_matrix)\n",
    "print(f\"Number of edges in the feature graph: {num_edges}\")\n",
    "np.savetxt('hyperlinks.csv', adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "zf = zipfile.ZipFile('hyperlinks.csv.zip') \n",
    "hyperlinks_df = pd.read_csv(zf.open('hyperlinks.csv'), header=None, sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlinks = hyperlinks_df.values\n",
    "plt.spy(hyperlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the laplacian\n",
    "laplacian = matrixu.compute_laplacian(hyperlinks, normalize=True)\n",
    "lam, U = matrixu.spectral_decomposition(np.nan_to_num(laplacian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.99 / np.max(lam)\n",
    "ideal_tk = np.reciprocal(1+alpha*lam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lam, ideal_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order =  10\n",
    "coeff = fit_polynomial(lam.real, order, ideal_filter)\n",
    "graph_filter = polynomial_graph_filter(coeff.real, laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_filter.T @ features.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pageviews of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix contain the total number of views during a period of 30 days for each articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df = aquire.page_view(list_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the words with the highest TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take all the words present in the articles that are not stopwords\n",
    "stop_found = ['====', '===', '==', '<<', '>>', \"''\", '``', \"'s\" , '\\displaystyle', '...', '\\phi', '\\mu', '\\mathbf', '--', 'x_',\n",
    "        '\\alpha', '\\dot', '\\hat', '\\lambda', '\\left', '\\right', 'mathcal', '\\nu', '\\partial'] #getting the stopwords found in the articles after exmination\n",
    "stop_pre = stopwords.words('english') # getting the common english stopwords\n",
    "stop_words = stop_found + stop_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_df = pd.read_pickle('./religion_articles')\n",
    "science_df = pd.read_pickle('./science_articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute only if you don't have the pickles\n",
    "religion_df= aquire.take_words(religion_articles, stop_words)\n",
    "religion_df = religion_df.drop_duplicates('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute only if you don't have the pickles\n",
    "science_df = aquire.take_words(science_articles, stop_words)\n",
    "science_df = science_df.drop_duplicates('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_labeled_df = religion_df.copy()\n",
    "religion_labeled_df['label'] = 0\n",
    "\n",
    "science_labeled_df = science_df.copy()\n",
    "science_labeled_df['label'] = 1\n",
    "\n",
    "# we select the same number of articles from both categories\n",
    "# we need a maximal number of data points for a good training process\n",
    "number_articles = min(len(religion_labeled_df), len(science_labeled_df))\n",
    "\n",
    "# we keep the first article which is the Science or Religion article and sample randomly from the rest\n",
    "science_labeled_df_sampled = pd.concat([pd.DataFrame(science_labeled_df.iloc[0]).T,\\\n",
    "           science_labeled_df.loc[1:].sample(n=number_articles - 1, random_state=1)],\\\n",
    "          ignore_index=True)\n",
    "\n",
    "religion_labeled_df_sampled = pd.concat([pd.DataFrame(religion_labeled_df.iloc[0]).T,\\\n",
    "           religion_labeled_df.loc[1:].sample(n=number_articles - 1, random_state=1)],\\\n",
    "          ignore_index=True)\n",
    "\n",
    "words_df_reduced = pd.concat([religion_labeled_df,\\\n",
    "                              science_labeled_df],\\\n",
    "                             ignore_index=True)\n",
    "                              \n",
    "labels_df = pd.DataFrame(words_df_reduced['label'])\n",
    "words_df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpu = NLPUtilities()\n",
    "# UNCOMMENT & EXECUTE IF YOU DO NOT HAVE tf_idf_50_words.pickle\n",
    "tf_idf = nlpu.TF_IDF(words_df_reduced, 50)\n",
    "#tf_idf = pd.read_pickle('tf_idf_50_words.pickle')\n",
    "#tf_idf\n",
    "important_words_df = pd.DataFrame(tf_idf.sum(axis=0, numeric_only=True).sort_values(ascending = False),\\\n",
    "                                  columns=['tfidf'])\n",
    "important_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted TF-IDF matrix $W_{w}$ is our tf_idf dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_weight = tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the non-weighted TF-IDF $W_{nw}$ where we compute the weights such that for all $i,j$\n",
    "\n",
    "$(W_{nw})_{i,j} = 1$ if $(W_{w})_{i,j} > 0$, $(W_{nw})_{i,j} = 0$ otherwise\n",
    "\n",
    "we can do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_no_weight = tf_idf.where(tf_idf <= 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this project we choose to work with the weighted TF-IDF matrix as it gives more insights into word importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have hence aquired the data and put it in a dataframe called features. Its rows are the different articles and each column is an important word (according to TF-IDF scores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import operator\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixUtils():\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_laplacian(adjacency: np.ndarray, normalize: bool):\n",
    "        \"\"\" \n",
    "        Function to compute the laplacian\n",
    "        Arguments:\n",
    "        adjacency - an adjacency matrix for which to compute the laplacian\n",
    "        normalize - if set to True, the normalized laplacian is returned, otherwise the combinatorial one is returned \n",
    "        Returns:\n",
    "        L (n x n ndarray): combinatorial or symmetric normalized Laplacian.\n",
    "        \"\"\"\n",
    "        degrees_l = np.sum(adjacency,axis=1)\n",
    "        if(not normalize):\n",
    "            degrees = np.zeros(adjacency.shape)\n",
    "            np.fill_diagonal(degrees,degrees_l)\n",
    "            return degrees - adjacency \n",
    "        else:\n",
    "            degrees = np.sqrt(np.array([degrees_l]).T @ np.array([degrees_l]))\n",
    "            L = - adjacency/degrees\n",
    "            np.fill_diagonal(L,np.ones(len(adjacency)))\n",
    "            return L\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectral_decomposition(laplacian: np.ndarray):\n",
    "        \"\"\"\n",
    "        Function to carry out spectral decomposition on a given matrix\n",
    "        Arguments:\n",
    "        laplacian - matrix on which to carry out decomposition\n",
    "        Returns:\n",
    "        lamb (np.array): eigenvalues of the Laplacian\n",
    "        U (np.ndarray): corresponding eigenvectors.\n",
    "        \"\"\"\n",
    "        values, vectors = np.linalg.eig(laplacian)\n",
    "        sorted_indices = np.argsort(values)\n",
    "        return values[sorted_indices], vectors[:,sorted_indices]\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine(a,b):\n",
    "        \"\"\"\n",
    "        Function to compute the cosine similarity between two vectors\n",
    "        Arguments:\n",
    "        a - first vector\n",
    "        b - second vector\n",
    "        Returns:\n",
    "        cos_sim - the cosine similarity between a and b\n",
    "        \"\"\"\n",
    "        if(norm(a) == 0 or norm(b) == 0):\n",
    "            return dot(a,b)\n",
    "        cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "        return cos_sim\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_distances(features):\n",
    "        features_a = np.array(features)\n",
    "\n",
    "        #compute cosine similarity between all articles\n",
    "        distances = np.array([[cosine(features_a[i,:],features_a[j,:]) for i in range(0,features_a.shape[0])]\n",
    "            for j in range(0,features_a.shape[0])]).reshape(features.shape[0],-1)\n",
    "\n",
    "        return distances\n",
    "    \n",
    "    @staticmethod\n",
    "    def article_sim_th(threshold, distances,weight=True):\n",
    "        \"\"\"\n",
    "        apply a threshold to cosine distances in order to get an adjacency matrix that \n",
    "        allow to observe the two classes of articles\n",
    "        \"\"\"\n",
    "        feat = distances.copy()\n",
    "        if(weight):\n",
    "            feat[feat<threshold]=0\n",
    "        else:\n",
    "            feat = np.where(distances > threshold, 1, 0)\n",
    "        np.fill_diagonal(feat, 0)\n",
    "        return feat\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_noconnected (adjacency, distances, tf_idf, labels_df, no_label=False):\n",
    "        nodes_df = pd.DataFrame(range(len(adjacency)), columns = [\"X\"])\n",
    "        nodes_df[\"Y\"] = np.zeros((len(adjacency)))\n",
    "        epsilon_df = pd.DataFrame(np.argwhere(adjacency != 0), columns = [\"X\",\"Y\"] ).drop_duplicates(subset = \"X\")\n",
    "        epsilon_wo = epsilon_df.merge(nodes_df, on = [\"X\"], how= \"right\")\n",
    "        removed_df = words_df_reduced[words_df_reduced[\"article\"].isin(tf_idf.iloc[epsilon_wo[pd.isna(epsilon_wo[\"Y_x\"]) == True][\"X\"]].index)]\n",
    "        distances_wo = np.delete(distances, epsilon_wo[pd.isna(epsilon_wo[\"Y_x\"]) == True][\"X\"], 0)\n",
    "        distances_wo = np.delete(distances_wo, epsilon_wo[pd.isna(epsilon_wo[\"Y_x\"]) == True][\"X\"], 1)\n",
    "        tf_idf_wo = tf_idf.drop(tf_idf.iloc[epsilon_wo[pd.isna(epsilon_wo[\"Y_x\"]) == True][\"X\"]].index.values)\n",
    "        if no_label == False:\n",
    "            labels_df_wo = labels_df.drop(labels_df.index[epsilon_wo[pd.isna(epsilon_wo[\"Y_x\"]) == True][\"X\"]]).reset_index()\n",
    "        else:\n",
    "            labels_df_wo = labels_df\n",
    "        return removed_df, distances_wo, tf_idf_wo, labels_df_wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisUtils():\n",
    "    \n",
    "    @staticmethod\n",
    "    def laplacian_eigenmaps(X:np.ndarray, dim: int, sigma: float, epsilon: float, normalize: bool):\n",
    "        \"\"\"\n",
    "        Function to compute the laplacian eigenmap if a given matrix\n",
    "        Arguments:\n",
    "        X - the matrix for which to compute the eigenmaps\n",
    "        dim - the dimension of the data we would like to return\n",
    "        sigma - the sigma parameter for the epsilon similarity graph\n",
    "        epsilon - the epsilon parameter for the epsilon similarity graph\n",
    "        normalize - if set to True, the normalized laplacian is used, otherwise the combinatorial one is used \n",
    "        Returns:\n",
    "        coords (n x dim array): new coordinates for the data points\n",
    "        \"\"\"\n",
    "        adjacency = MatrixUtils().epsilon_similarity_graph(X, sigma, epsilon)\n",
    "        laplacian = MatrixUtils().compute_laplacian(adjacency, normalize)\n",
    "        eigenvalues, eigenvectors = MatrixUtils().spectral_decomposition(np.nan_to_num(laplacian)) \n",
    "        return (eigenvectors[:,1:dim+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/ Build an adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ Compute distances between articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixu = MatrixUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_a = np.array(features)\n",
    "distances = cosine_distances(features_a)\n",
    "distances_religion = matrixu.cosine_distances(features_a[labels_df_wo[labels_df_wo[\"label\"] == 0].index])\n",
    "distances_science = matrixu.cosine_distances(features_a[labels_df_wo[labels_df_wo[\"label\"] == 1].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of cosine similarity\n",
    "\n",
    "plt.figure(1, figsize=(8, 4))\n",
    "plt.title(\"Histogram of cosine similarity between pages\")\n",
    "plt.hist(distances .flatten(),bins =50);\n",
    "plt.xlabel('cosine similarity')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "\n",
    "quantile = 0.95\n",
    "print(f\"{quantile*100}% of the distances between pages are below: \", np.quantile(distances,quantile))\n",
    "\n",
    "plt.title(\"Histogram of cosine similarity inside of a category\")\n",
    "plt.hist(distances_science.flatten(), bins = 50 , alpha=0.5, label='science')\n",
    "plt.hist(distances_religion.flatten(), bins = 50, alpha=0.5, label='religion')\n",
    "plt.xlabel('cosine similarity')\n",
    "plt.ylabel('count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "print(f\"{quantile*100}% of the distances between religion pages are below: \",  np.quantile(distances_religion,quantile))\n",
    "print(f\"{quantile*100}% of the distances between science pages are below: \", np.quantile(distances_science,quantile))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a heavy-tailed distribution. The similarity between papers is hence concentrated around 0 and 0.2 between most papers but there are some rare papers with very high cosine similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine an example of distance distribution for one religon paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(distances[0],'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine an example of distance distribution for one scientific paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(distances[1200],'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/ Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to previous part approximatly 95% of the articles of given category have cosine distances above 0.6\n",
    "# we will keep this value as a threshold, distances that are below 0.6, won't be taken in account in our adjacency matrix\n",
    "adjacency = matrixu.article_sim_th(0.65,distances, weight=True)\n",
    "\n",
    "plt.spy(adjacency)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B/ Exploring graph properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ General features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a graph using the normalized distances\n",
    "graph = nx.from_numpy_matrix(adjacency)\n",
    "\n",
    "# assessing the giant component size\n",
    "giant_feature = max(nx.connected_component_subgraphs(graph), key=len)\n",
    "print('The giant component of the feature graph has {} nodes and {} edges.'.format(giant_feature.number_of_nodes(), giant_feature.size()))\n",
    "\n",
    "print(\"average clustering coefficient of our graph:\",nx.average_clustering(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study the connected components\n",
    "graphs = list(nx.connected_component_subgraphs(graph))\n",
    "for i in range(0,len(graphs)):\n",
    "    print(\"subgraph nb \", i, \"nb of nodes:\",len(graphs[i].degree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph has one giant component and a small component composed of 2 nodes, the 16 other components are pages that are alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_small_comp = list(graphs[11].nodes())\n",
    "\n",
    "#for curiosity, articles linked in the small component\n",
    "for node in nodes_small_comp:\n",
    "     print(features.index[node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/ Degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Degree distribution\")\n",
    "degrees = [d for (i,d) in graph.degree()]\n",
    "plt.hist(er_degrees,bins = 50);\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the degree follows a power law, hence the garph corresponds to a scale free network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observe articles with highest degree\n",
    "\n",
    "list_degree = list(graph.degree)\n",
    "degree_religion = []\n",
    "degree_science = []\n",
    "data= []\n",
    "\n",
    "for node in list_degree:\n",
    "    if(node[0]<=751):\n",
    "        category = \"religion\"\n",
    "        degree_religion.append(node[1])\n",
    "    else:\n",
    "        category = \"science\"\n",
    "        degree_science.append(node[1])\n",
    "    article_name = features.index[node[0]]\n",
    "    degree = node[1]\n",
    "    \n",
    "    data.append([article_name,category,degree])\n",
    "    \n",
    "df = pd.DataFrame(data, columns = ['Article Name', 'Category','Degree'])\n",
    "\n",
    "plt.title(\"Degree distribution by category\")\n",
    "plt.hist(degree_religion, bins = 50 , alpha=0.5, label='religion')\n",
    "plt.hist(degree_science, bins = 50, alpha=0.5, label='science')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/ Articles with highest and lowest degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles with highest degree\n",
    "df.sort_values(by=['Degree'],ascending=False)[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring possible clustering in the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the nodes that are not connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_df, distances_wo, tf_idf_wo, labels_df_wo = matrixu.remove_noconnected(adjacency, distances, tf_idf, labels_df)\n",
    "removed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nb_words_removed = len(removed_df[\"words\"].explode())/len(removed_df)\n",
    "print(\"The mean number of words of the removed articles are \" + str(mean_nb_words_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df_removed = views_df.merge(removed_df, on = \"article\")\n",
    "mean_nb_views_removed = views_df_removed[\"views\"].sum()/len(views_df_removed)\n",
    "print(\"The mean number of views of these articles are \" + str(mean_nb_views_removed) + \" per month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nb_words = len(words_df_reduced[\"words\"].explode())/len(words_df_reduced)\n",
    "print(\"The mean number of words of the articles are \" + str(mean_nb_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nb_views = views_df[\"views\"].sum()/len(views_df)\n",
    "print(\"The mean number of views of the articles are \" + str(mean_nb_views) + \" per month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The removed articles belong to the religon and science articles with approximatively the same proportion, excluding a possible bias that could happen when removing articles. Furthermore, these articles are mostly short articles. We can claim that these articles are stub and need more expansion and devolepment. The mean number of views for the removed articles are way less than for the total articles, adding to the supposition that htese articles are stubs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_wo = article_sim_th(0.50,distances_wo, weight=True)\n",
    "\n",
    "plt.spy(adjacency_wo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the graph obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu = VisUtils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian Eigenmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = matrixu.compute_laplacian(adjacency_wo, normalize=True)\n",
    "eigenvalues, eigenvectors = matrixu.spectral_decomposition(np.nan_to_num(laplacian)) \n",
    "twoD_embeddings = (eigenvectors[:,1:3])\n",
    "plt.scatter(twoD_embeddings[labels_df_wo[labels_df_wo[\"label\"] == 0].index,0], twoD_embeddings[labels_df_wo[labels_df_wo[\"label\"] == 0].index,1], label=1)\n",
    "plt.scatter(twoD_embeddings[labels_df_wo[labels_df_wo[\"label\"] == 1].index,0], twoD_embeddings[labels_df_wo[labels_df_wo[\"label\"] == 1].index,1], label=2)\n",
    "plt.title('Graph of Wikipedia Science and Religion articles (done using Laplacian Eigenmaps)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components = 2)\n",
    "x_embed = tsne.fit_transform(distances_wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_embed[labels_df_wo[labels_df_wo[\"label\"] == 0].index,0], x_embed[labels_df_wo[labels_df_wo[\"label\"] == 0].index,1], label=1)\n",
    "plt.scatter(x_embed[labels_df_wo[labels_df_wo[\"label\"] == 1].index,0], x_embed[labels_df_wo[labels_df_wo[\"label\"] == 1].index,1], label=2)\n",
    "plt.title('Graph of Wikipedia Science and Religion articles (done using tSN-E)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic Eigengaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = matrixu.compute_laplacian(adjacency_wo, normalize = True)\n",
    "eigenvalues, eigenvectors = matrixu.spectral_decomposition(laplacian)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(eigenvalues[0:20])\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Eigenvalues $L_{comb}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heuristic eigengaps state that the number of clusters need to be equal to the index of eigenvalues with the largest difference between themselves. However, here there is no clear cut except maybe for the index 2 that have a large eigengap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralClustering():\n",
    "    def __init__(self, n_classes: int, normalize: bool):\n",
    "        self.n_classes = n_classes\n",
    "        self.normalize = normalize\n",
    "        self.laplacian = None\n",
    "        self.e = None\n",
    "        self.U = None\n",
    "        self.clustering_method =  None\n",
    "        \n",
    "    def fit_predict(self, adjacency):\n",
    "        \"\"\" Your code should be correct both for the combinatorial\n",
    "            and the symmetric normalized spectral clustering.\n",
    "            Return:\n",
    "            y_pred (np.ndarray): cluster assignments.\n",
    "        \"\"\"\n",
    "        laplacian = matrixu.compute_laplacian(adjacency, self.normalize)\n",
    "        lamb, U = matrixu.spectral_decomposition(laplacian)\n",
    "        Y = U[:,0:self.n_classes].real\n",
    "        kmeans = KMeans(n_clusters = self.n_classes).fit(Y)\n",
    "        y_pred =  kmeans.predict(Y)\n",
    "        centers = kmeans.cluster_centers_\n",
    "        return y_pred, centers\n",
    "    \n",
    "    def make_clusters(self, adjacency, tf_idf_wo, df) : \n",
    "        y_pred, centers = self.fit_predict(adjacency)\n",
    "        y_pred = pd.DataFrame(y_pred, columns = [\"label\"])\n",
    "        laplacian = matrixu.compute_laplacian(adjacency, True)\n",
    "        lamb, U = matrixu.spectral_decomposition(laplacian)\n",
    "        Y = U[:,0:self.n_classes].real\n",
    "        clusters_df = pd.DataFrame([tf_idf_wo.columns.values[np.argsort(-(Y @ centers.T).T@tf_idf_wo.values, 1)[i][:10]] for i in range(self.n_classes)]).T\n",
    "        clusters_df.index = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "        clusters_df.loc[0] = tf_idf_wo.index[np.argmax(Y @ centers.T, 0)]\n",
    "        clusters_df.loc[1] = [df.merge(tf_idf_wo.iloc[y_pred[y_pred[\"label\"] == i].index], on = \"article\").explode(\"words\").groupby([\"article\"]).count().sort_values(by = [\"words\"], ascending = False).index[0] for i in range(self.n_classes)]\n",
    "        clusters_df.loc[2] = [views_df.merge(tf_idf_wo.iloc[y_pred[y_pred[\"label\"] == i].index], on = \"article\").sort_values(by = [\"views\"], ascending = False).iloc[0][\"article\"] for i in range(self.n_classes)]\n",
    "        clusters_df.loc[13] = [len(y_pred[y_pred[\"label\"] == i]) for i in range(self.n_classes)]\n",
    "        clusters_df.loc[14] = [100 - (labels_df_wo.iloc[y_pred[y_pred[\"label\"] == i].index].sum().values[-1:]*100 / len(y_pred[y_pred[\"label\"] == i])) for i in range(self.n_classes)]\n",
    "        clusters_df.loc[15] = [labels_df_wo.iloc[y_pred[y_pred[\"label\"] == i].index].sum().values[-1:]*100 / len(y_pred[y_pred[\"label\"] == i]) for i in range(self.n_classes)]\n",
    "        clustering = [0 for  i in range(self.n_classes)]\n",
    "        for i in range(self.n_classes):\n",
    "            graph = nx.from_numpy_matrix(hyperlinks_df.iloc[views_df.reset_index().merge(tf_idf_wo.iloc[y_pred[y_pred[\"label\"] == i].index], on = \"article\").index,views_df.reset_index().merge(tf_idf_wo.iloc[y_pred[y_pred[\"label\"] == 0].index], on = \"article\").index].values)\n",
    "            clustering[i] = nx.average_clustering(graph)\n",
    "        clusters_df.loc[15] = clustering\n",
    "        clusters_df.sort_index(inplace=True)\n",
    "        clusters_df.index = [\"Most relevant article\", \"Longest article\", \"Most viewed article\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"Number of articles\",\"Percentage of religion article\", \"Percentage of science articles\", \"Average clustering of the hyperlinks matrix\"]\n",
    "        clusters_df.sort_values(by = [\"Number of articles\"], axis = 1, inplace = True)\n",
    "        return clusters_df, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_clustering = SpectralClustering(n_classes=2, normalize=True)\n",
    "clusters_df, y_pred = spectral_clustering.make_clusters(adjacency_wo, tf_idf_wo, words_df_reduced)\n",
    "clusters_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two clusters seem to differentiate : one cluster with approximatively only religion articles, the other one with approximatively all the science articles plus a part of religion articles. What characterized these religions articles. To have a closer look we perform the entire pipeline with only this subset of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_interest_df = y_pred[y_pred[\"label\"] == 0].merge(labels_df_wo[labels_df_wo[\"label\"] == 0], left_index = True, right_index = True)\n",
    "new_df = words_df_reduced.merge(new_interest_df, left_index = True, right_on = \"index\")\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entire_pipeline(df, nb_clusters=2):\n",
    "    tf_idf = nlpu.TF_IDF(df, 50)\n",
    "    features = tf_idf\n",
    "    important_words_df = pd.DataFrame(tf_idf.sum(axis=0, numeric_only=True).sort_values(ascending = False),\\\n",
    "                                  columns=['tfidf'])\n",
    "    features_a = np.array(features)\n",
    "    distances = cosine_distances(features_a)\n",
    "    distances = matrixu.cosine_distances(features_a)\n",
    "    adjacency = matrixu.article_sim_th(0.60,distances, weight=True)\n",
    "    removed_df, distances_wo, tf_idf_wo, new_labels_df_wo = matrixu.remove_noconnected(adjacency, distances, tf_idf, labels_df_wo)\n",
    "    adjacency_wo = article_sim_th(0.50,distances_wo, weight=True)\n",
    "    laplacian = matrixu.compute_laplacian(adjacency_wo, normalize = True)\n",
    "    eigenvalues, eigenvectors = matrixu.spectral_decomposition(laplacian)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(eigenvalues[0:20])\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Eigenvalue')\n",
    "    plt.title('Eigenvalues $L_{comb}$')\n",
    "    spectral_clustering = SpectralClustering(n_classes=nb_clusters, normalize=True)\n",
    "    clusters_df, y_pred = spectral_clustering.make_clusters(adjacency_wo, tf_idf_wo, new_df)\n",
    "    return clusters_df, y_pred, important_words_df, removed_df, new_labels_df_wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters_df, new_y_pred, new_important_words_df, new_removed, new_labels_df_wo = entire_pipeline(new_df, 6)\n",
    "new_clusters_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 major clusters : one representing the philosophy and spirituality, another one which include the fundation of research in science and religion, one which correspond more of stubs articles that wasn't removed, one about the list of angels and the final one about theosophy which is a new religion movement launched by Helena Blavatsky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = words_df_reduced.merge(pd.concat([removed_df, new_removed]), on = [\"article\",\"label\",\"words_string\"], how = \"left\", indicator=True)\n",
    "df = df[df[\"_merge\"]=='left_only'].drop(columns = [\"words_y\", \"_merge\"])\n",
    "df.columns = [\"article\", \"words\", \"label\", \"words_string\"]\n",
    "words = pd.concat([important_words_df, new_important_words_df]).index\n",
    "final_features = nlpu.TF_IDF(df, 100, True, words.values)\n",
    "features_a = np.array(final_features)\n",
    "distances = cosine_distances(features_a)\n",
    "distances = matrixu.cosine_distances(features_a)\n",
    "final_adjacency = matrixu.article_sim_th(0.50,distances, weight=True)\n",
    "plt.spy(final_adjacency)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.DataFrame(df[df[\"article\"].isin(tf_idf_wo.iloc[y_pred[y_pred[\"label\"] == 0].index].index)].index.values, columns = [\"Index\"])\n",
    "df0[\"label\"] = 0\n",
    "df0[\"name\"] = \"This article is a religion related articles\"\n",
    "df1 = pd.DataFrame(df[df[\"article\"].isin(tf_idf_wo.iloc[y_pred[y_pred[\"label\"] == 1].index].index)].index.values, columns = [\"Index\"])\n",
    "df1[\"label\"] = 1\n",
    "df1[\"name\"] = \"This article is a science related articles\"\n",
    "df2 = pd.DataFrame(df[df[\"article\"].isin(new_tf_idf_wo.iloc[new_y_pred[new_y_pred[\"label\"] == 0].index].index)].index.values, columns = [\"Index\"])\n",
    "df2[\"label\"] = 2\n",
    "df2[\"name\"] = \"This article is a Center for Inquiry related articles\"\n",
    "df3 = pd.DataFrame(df[df[\"article\"].isin(new_tf_idf_wo.iloc[new_y_pred[new_y_pred[\"label\"] == 1].index].index)].index.values, columns = [\"Index\"])\n",
    "df3[\"label\"] = 3\n",
    "df3[\"name\"] = \"This article is a theosophy related articles\"\n",
    "df4 = pd.DataFrame(df[df[\"article\"].isin(new_tf_idf_wo.iloc[new_y_pred[new_y_pred[\"label\"] == 2].index].index)].index.values, columns = [\"Index\"])\n",
    "df4[\"label\"] = 4\n",
    "df4[\"name\"] = \"This article is a list in religions related articles\"\n",
    "df5 = pd.DataFrame(df[df[\"article\"].isin(new_tf_idf_wo.iloc[new_y_pred[new_y_pred[\"label\"] == 3].index].index)].index.values, columns = [\"Index\"])\n",
    "df5[\"label\"] = 5\n",
    "df5[\"name\"] = \"This article is a faith related articles\" \n",
    "df6 = pd.DataFrame(df[df[\"article\"].isin(new_tf_idf_wo.iloc[new_y_pred[new_y_pred[\"label\"] == 4].index].index)].index.values, columns = [\"Index\"])\n",
    "df6[\"label\"] = 6\n",
    "df6[\"name\"] = \"This article is a Old and New Testament related articles\"\n",
    "df7 = pd.DataFrame(df[df[\"article\"].isin(new_tf_idf_wo.iloc[new_y_pred[new_y_pred[\"label\"] == 5].index].index)].index.values, columns = [\"Index\"])\n",
    "df7[\"label\"] = 7\n",
    "df7[\"name\"] = \"This article is a spirituality, psychology and comedy about religion related articles\"\n",
    "labels = pd.concat([df0, df1, df2, df3, df4, df5, df6, df7]).drop_duplicates(subset = \"Index\", keep = \"last\").sort_values(by = ['Index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacianPolynomial(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 k: int,\n",
    "                 dropout_prob: float,\n",
    "                 norm=True):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._k = k\n",
    "        self._norm = norm\n",
    "        # Contains the weights learned by the Laplacian polynomial\n",
    "        self.pol_weights = nn.Parameter(torch.Tensor(self._k + 1))\n",
    "        # Contains the weights learned by the logistic regression (without bias)\n",
    "        self.logr_weights = nn.Parameter(torch.Tensor(in_feats, out_feats))\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        torch.nn.init.xavier_uniform_(self.logr_weights, gain=0.01)\n",
    "        torch.nn.init.normal_(self.pol_weights, mean=0.0, std=1e-3)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        r\"\"\"Compute graph convolution.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        * Input shape: :math:`(N, *, \\text{in_feats})` where * means any number of additional\n",
    "          dimensions, :math:`N` is the number of nodes.\n",
    "        * Output shape: :math:`(N, *, \\text{out_feats})` where all but the last dimension are\n",
    "          the same shape as the input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph (DGLGraph) : The graph.\n",
    "        feat (torch.Tensor): The input feature\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (torch.Tensor) The output feature\n",
    "        \"\"\"\n",
    "        feat = self.dropout(feat)\n",
    "        graph = graph.local_var()\n",
    "        \n",
    "        # D^(-1/2)\n",
    "        norm = torch.pow(graph.in_degrees().float().clamp(min=1), -0.5)\n",
    "        shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "        norm = torch.reshape(norm, shp)\n",
    "\n",
    "        # mult W first to reduce the feature size for aggregation.\n",
    "        feat = torch.matmul(feat, self.logr_weights)\n",
    "\n",
    "        result = self.pol_weights[0] * feat.clone()\n",
    "\n",
    "        for i in range(1, self._k + 1):\n",
    "            old_feat = feat.clone()\n",
    "            if self._norm:\n",
    "                feat = feat * norm\n",
    "            graph.ndata['h'] = feat\n",
    "            # Feat is not modified in place\n",
    "            graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                             fn.sum(msg='m', out='h'))\n",
    "            if self._norm:\n",
    "                graph.ndata['h'] = graph.ndata['h'] * norm\n",
    "\n",
    "            feat = old_feat - graph.ndata['h']\n",
    "            result += self.pol_weights[i] * feat\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"Set the extra representation of the module,\n",
    "        which will come into effect when printing the model.\n",
    "        \"\"\"\n",
    "        summary = 'in={_in_feats}, out={_out_feats}'\n",
    "        summary += ', normalization={_norm}'\n",
    "        return summary.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, features, labels, loss_fcn, train_mask, optimizer):\n",
    "    model.train()  # Activate dropout\n",
    "    \n",
    "    logits = model(g, features)\n",
    "    loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()  # Deactivate dropout\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)[mask]  # only compute the evaluation set\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "       \n",
    "def test_articles(model, g, features, mask):\n",
    "    model.eval()  # Deactivate dropout\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)[mask] \n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        return indices\n",
    "    \n",
    "def make_predictions(list_articles, stop_words, df, words):\n",
    "    test_df = aquire.take_words(list_articles, stop_words, is_title = True)\n",
    "    predict_df = pd.concat([df[[\"article\", \"words\"]],test_df])\n",
    "    features_final = nlpu.TF_IDF(predict_df, 100, True, words.values)\n",
    "    features_a = np.array(features_final)\n",
    "    distances = matrixu.cosine_distances(features_a)\n",
    "    adjacency = matrixu.article_sim_th(0.70,distances, weight=True)\n",
    "    removed_df, distances_wo, tf_idf_wo, new_labels_df_wo = matrixu.remove_noconnected(adjacency, distances, features_final, labels_df_wo)\n",
    "    \n",
    "    if(len(list(set(removed_df[\"article\"].tolist()).intersection(list_articles))) != 0):\n",
    "        idx = [list_articles.index(list(set(removed_df[\"article\"].tolist()).intersection(list_articles))[j]) for j in range(len(list(set(removed_df[\"article\"].tolist()).intersection(list_articles))))]\n",
    "        \n",
    "        \n",
    "    adjacency = matrixu.article_sim_th(0.50,distances, weight=True)\n",
    "    graph = nx.from_numpy_array(adjacency)\n",
    "    graph = DGLGraph(graph)\n",
    "    features_ = torch.FloatTensor(features_a)\n",
    "    mask1 = np.zeros((len(df)))\n",
    "    mask2 = np.ones((len(list_articles)))\n",
    "    mask = np.concatenate((mask1,mask2))\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    results = test_articles(model, graph, features_, mask).tolist()\n",
    "    if (len(list(set(removed_df[\"article\"].tolist()).intersection(list_articles))) != 0):\n",
    "        for k in range(len(idx)):\n",
    "            results[idx[k]] = \"This article is either not connected to religion or science domains or a stub\"\n",
    "    class_df = pd.DataFrame(np.column_stack((list_articles,\\\n",
    "                                         [labels[labels[\"label\"] == results[i]][\"name\"].drop_duplicates().values[0] for i in range(len(list_articles))])),\\\n",
    "                                           columns = [\"Name of the Article\", \"Prediction\"])\n",
    "    return class_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification (Science vs Religion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x1, x2, y1, y2, idx1, idx2 = train_test_split(\n",
    "    final_adjacency, new_labels_df_wo['label'], range(final_adjacency.shape[1]), test_size=0.4)\n",
    "x3, x4, y3, y4, idx3, idx4 = train_test_split(\n",
    "    x2, y2, idx2, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(np.column_stack((idx1, np.ones(len(idx1)).T)), columns = ['idx', 'indices'])\n",
    "train_ = new_labels_df_wo.merge(train_df, left_index = True, right_on = 'idx', how = 'left').fillna(0)\n",
    "val_df = pd.DataFrame(np.column_stack((idx3, np.ones(len(idx3)).T)), columns = ['idx', 'indices'])\n",
    "val_ = new_labels_df_wo.merge(val_df, left_index = True, right_on = 'idx', how = 'left').fillna(0)\n",
    "test_df = pd.DataFrame(np.column_stack((idx4, np.ones(len(idx4)).T)), columns = ['idx', 'indices'])\n",
    "test_ = new_labels_df_wo.merge(val_df, left_index = True, right_on = 'idx', how = 'left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = final_features.shape[1] \n",
    "n_classes = 2\n",
    "pol_order = 3\n",
    "lr = 0.2\n",
    "weight_decay = 5e-6\n",
    "n_epochs = 100\n",
    "p_dropout = 0.8\n",
    "features_ = torch.FloatTensor(final_features.values) \n",
    "final_labels = torch.LongTensor(new_labels_df_wo['label'].values) \n",
    "train_mask = torch.BoolTensor(train_['indices'].values)\n",
    "val_mask = torch.BoolTensor(val_['indices'].values)\n",
    "test_mask = torch.BoolTensor(test_['indices'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LaplacianPolynomial(in_feats, n_classes, pol_order, p_dropout)\n",
    "\n",
    "graph_nx = nx.from_numpy_matrix(final_adjacency)\n",
    "graph = DGLGraph(graph_nx)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    loss = train(model, graph, features_, final_labels, loss_fcn, train_mask, optimizer)\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(model, graph, features_, final_labels, val_mask)\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Train Loss {:.4f} | Val Accuracy {:.4f}\". format(\n",
    "            epoch, np.mean(dur), loss.item(), acc))\n",
    "\n",
    "print()\n",
    "acc = evaluate(model, graph, features_, final_labels, test_mask)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x1, x2, y1, y2, idx1, idx2 = train_test_split(\n",
    "    final_adjacency, labels['label'], range(final_adjacency.shape[1]), test_size=0.4)\n",
    "x3, x4, y3, y4, idx3, idx4 = train_test_split(\n",
    "    x2, y2, idx2, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(np.column_stack((idx1, np.ones(len(idx1)).T)), columns = ['idx', 'indices'])\n",
    "train_ = labels.merge(train_df, left_index = True, right_on = 'idx', how = 'left').fillna(0)\n",
    "val_df = pd.DataFrame(np.column_stack((idx3, np.ones(len(idx3)).T)), columns = ['idx', 'indices'])\n",
    "val_ = labels.merge(val_df, left_index = True, right_on = 'idx', how = 'left').fillna(0)\n",
    "test_df = pd.DataFrame(np.column_stack((idx4, np.ones(len(idx4)).T)), columns = ['idx', 'indices'])\n",
    "test_ = labels.merge(val_df, left_index = True, right_on = 'idx', how = 'left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = final_features.shape[1] \n",
    "n_classes = 8\n",
    "pol_order = 3\n",
    "lr = 0.2\n",
    "weight_decay = 5e-6\n",
    "n_epochs = 100\n",
    "p_dropout = 0.8\n",
    "features_ = torch.FloatTensor(final_features.values) \n",
    "final_labels = torch.LongTensor(labels['label'].values) \n",
    "train_mask = torch.BoolTensor(train_['indices'].values)\n",
    "val_mask = torch.BoolTensor(val_['indices'].values)\n",
    "test_mask = torch.BoolTensor(test_['indices'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LaplacianPolynomial(in_feats, n_classes, pol_order, p_dropout)\n",
    "\n",
    "graph_nx = nx.from_numpy_matrix(final_adjacency)\n",
    "graph = DGLGraph(graph_nx)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    loss = train(model, graph, features_, final_labels, loss_fcn, train_mask, optimizer)\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(model, graph, features_, final_labels, val_mask)\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Train Loss {:.4f} | Val Accuracy {:.4f}\". format(\n",
    "            epoch, np.mean(dur), loss.item(), acc))\n",
    "\n",
    "print()\n",
    "acc = evaluate(model, graph, features_, final_labels, test_mask)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_articles = [\"God\", \"Network Science\", \"Taylor Swift\", \"Helena Blavatsky\", \"Spirituality\", \"Kylian Mbappe\", \"Ezekiel\", \"Faith\", \"Bristol\",\\\n",
    "                 \"List of angels in theology\", \"Nabeul\", \"Committee for Skeptical Inquiry\"]\n",
    "make_predictions(list_articles, stop_words, df, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
